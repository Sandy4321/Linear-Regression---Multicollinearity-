{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import statsmodels.api as sm \n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.stats import t as tdist \n",
    "import scipy.stats as stats\n",
    "from statsmodels.stats.outliers_influence import summary_table\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from patsy import dmatrices\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn import datasets, linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/sahiljain/Downloads'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/sahiljain/Downloads/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hitters = pd.read_csv('hitters.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "(A) Calculate the VIF for each of the explanatory variables. Comment \n",
    "on weather multicollinearity appears to be an isse. If it is,\n",
    "identify the three explanatory variables that are most seriously \n",
    "affected by the issue. \n",
    "\n",
    "First we will do a MLR of the data, then calculate the VIF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y, X = dmatrices('Salary ~ AtBat + Hits + HmRun +  Runs + RBI + Walks + Years + CAtBat + CHits + CHmRun + CRuns + CRBI + CWalks + League + Division + PutOuts + Assists + Errors + NewLeague', data=hitters, return_type=\"dataframe\")\n",
    "vif = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "result = sm.OLS(y, X).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>Salary</td>      <th>  R-squared:         </th> <td>   0.546</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.511</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   15.39</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 21 Nov 2017</td> <th>  Prob (F-statistic):</th> <td>7.84e-32</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>18:55:42</td>     <th>  Log-Likelihood:    </th> <td> -1876.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   263</td>      <th>  AIC:               </th> <td>   3792.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   243</td>      <th>  BIC:               </th> <td>   3864.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    19</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>           <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>      <td>  163.1036</td> <td>   90.779</td> <td>    1.797</td> <td> 0.074</td> <td>  -15.710</td> <td>  341.917</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>League[T.N]</th>    <td>   62.5994</td> <td>   79.261</td> <td>    0.790</td> <td> 0.430</td> <td>  -93.528</td> <td>  218.727</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Division[T.W]</th>  <td> -116.8492</td> <td>   40.367</td> <td>   -2.895</td> <td> 0.004</td> <td> -196.363</td> <td>  -37.335</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>NewLeague[T.N]</th> <td>  -24.7623</td> <td>   79.003</td> <td>   -0.313</td> <td> 0.754</td> <td> -180.380</td> <td>  130.855</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AtBat</th>          <td>   -1.9799</td> <td>    0.634</td> <td>   -3.123</td> <td> 0.002</td> <td>   -3.229</td> <td>   -0.731</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Hits</th>           <td>    7.5008</td> <td>    2.378</td> <td>    3.155</td> <td> 0.002</td> <td>    2.818</td> <td>   12.184</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HmRun</th>          <td>    4.3309</td> <td>    6.201</td> <td>    0.698</td> <td> 0.486</td> <td>   -7.885</td> <td>   16.546</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Runs</th>           <td>   -2.3762</td> <td>    2.981</td> <td>   -0.797</td> <td> 0.426</td> <td>   -8.248</td> <td>    3.495</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RBI</th>            <td>   -1.0450</td> <td>    2.601</td> <td>   -0.402</td> <td> 0.688</td> <td>   -6.168</td> <td>    4.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Walks</th>          <td>    6.2313</td> <td>    1.829</td> <td>    3.408</td> <td> 0.001</td> <td>    2.630</td> <td>    9.833</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Years</th>          <td>   -3.4891</td> <td>   12.412</td> <td>   -0.281</td> <td> 0.779</td> <td>  -27.938</td> <td>   20.960</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CAtBat</th>         <td>   -0.1713</td> <td>    0.135</td> <td>   -1.267</td> <td> 0.206</td> <td>   -0.438</td> <td>    0.095</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CHits</th>          <td>    0.1340</td> <td>    0.675</td> <td>    0.199</td> <td> 0.843</td> <td>   -1.195</td> <td>    1.463</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CHmRun</th>         <td>   -0.1729</td> <td>    1.617</td> <td>   -0.107</td> <td> 0.915</td> <td>   -3.358</td> <td>    3.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CRuns</th>          <td>    1.4543</td> <td>    0.750</td> <td>    1.938</td> <td> 0.054</td> <td>   -0.024</td> <td>    2.933</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CRBI</th>           <td>    0.8077</td> <td>    0.693</td> <td>    1.166</td> <td> 0.245</td> <td>   -0.557</td> <td>    2.172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CWalks</th>         <td>   -0.8116</td> <td>    0.328</td> <td>   -2.474</td> <td> 0.014</td> <td>   -1.458</td> <td>   -0.165</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PutOuts</th>        <td>    0.2819</td> <td>    0.077</td> <td>    3.640</td> <td> 0.000</td> <td>    0.129</td> <td>    0.434</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Assists</th>        <td>    0.3711</td> <td>    0.221</td> <td>    1.678</td> <td> 0.095</td> <td>   -0.065</td> <td>    0.807</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Errors</th>         <td>   -3.3608</td> <td>    4.392</td> <td>   -0.765</td> <td> 0.445</td> <td>  -12.011</td> <td>    5.290</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>87.414</td> <th>  Durbin-Watson:     </th> <td>   2.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 452.923</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.236</td> <th>  Prob(JB):          </th> <td>4.46e-99</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 8.934</td> <th>  Cond. No.          </th> <td>2.09e+04</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 Salary   R-squared:                       0.546\n",
       "Model:                            OLS   Adj. R-squared:                  0.511\n",
       "Method:                 Least Squares   F-statistic:                     15.39\n",
       "Date:                Tue, 21 Nov 2017   Prob (F-statistic):           7.84e-32\n",
       "Time:                        18:55:42   Log-Likelihood:                -1876.2\n",
       "No. Observations:                 263   AIC:                             3792.\n",
       "Df Residuals:                     243   BIC:                             3864.\n",
       "Df Model:                          19                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==================================================================================\n",
       "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------\n",
       "Intercept        163.1036     90.779      1.797      0.074     -15.710     341.917\n",
       "League[T.N]       62.5994     79.261      0.790      0.430     -93.528     218.727\n",
       "Division[T.W]   -116.8492     40.367     -2.895      0.004    -196.363     -37.335\n",
       "NewLeague[T.N]   -24.7623     79.003     -0.313      0.754    -180.380     130.855\n",
       "AtBat             -1.9799      0.634     -3.123      0.002      -3.229      -0.731\n",
       "Hits               7.5008      2.378      3.155      0.002       2.818      12.184\n",
       "HmRun              4.3309      6.201      0.698      0.486      -7.885      16.546\n",
       "Runs              -2.3762      2.981     -0.797      0.426      -8.248       3.495\n",
       "RBI               -1.0450      2.601     -0.402      0.688      -6.168       4.078\n",
       "Walks              6.2313      1.829      3.408      0.001       2.630       9.833\n",
       "Years             -3.4891     12.412     -0.281      0.779     -27.938      20.960\n",
       "CAtBat            -0.1713      0.135     -1.267      0.206      -0.438       0.095\n",
       "CHits              0.1340      0.675      0.199      0.843      -1.195       1.463\n",
       "CHmRun            -0.1729      1.617     -0.107      0.915      -3.358       3.013\n",
       "CRuns              1.4543      0.750      1.938      0.054      -0.024       2.933\n",
       "CRBI               0.8077      0.693      1.166      0.245      -0.557       2.172\n",
       "CWalks            -0.8116      0.328     -2.474      0.014      -1.458      -0.165\n",
       "PutOuts            0.2819      0.077      3.640      0.000       0.129       0.434\n",
       "Assists            0.3711      0.221      1.678      0.095      -0.065       0.807\n",
       "Errors            -3.3608      4.392     -0.765      0.445     -12.011       5.290\n",
       "==============================================================================\n",
       "Omnibus:                       87.414   Durbin-Watson:                   2.018\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              452.923\n",
       "Skew:                           1.236   Prob(JB):                     4.46e-99\n",
       "Kurtosis:                       8.934   Cond. No.                     2.09e+04\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 2.09e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21.762082248706328, 4.1341152908350125, 1.0753975007072221, 4.0990634495651141, 22.944365916840002, 30.281255330499725, 7.7586678548431989, 15.246417502100771, 11.9217150823485, 4.1487119716733201, 9.3132799023356263, 251.5611595648281, 502.95428903838939, 46.488461529704331, 162.52081015034895, 131.96585767572046, 19.744105013833646, 1.2363169622521883, 2.7093409367595696, 2.2145434666868042]\n"
     ]
    }
   ],
   "source": [
    "print vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, multicolinearity does appear to be an issue here as some of the \n",
    "variables are seriously affected. Most seriously affected variables \n",
    "are :\n",
    "(1) CAtBat \n",
    "(2) CHits \n",
    "(3) CRuns\n",
    "If we take a closer look, Python also calculates the VIF for categorical variables as well as for the intercept so the first four outputs in the vif are of categorical and response variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(F) In this part you will compare the predictive performance of four \n",
    "models:\n",
    "\n",
    "(i) The full model with all 19 explanatory variables.\n",
    "(ii) The optimal model identified in part (b).\n",
    "(iii) The best model from parts (c)-(e) (i.e., the best stepwise-selection model).\n",
    "(iv) The model that is considered optimal with respect to the Bayesian\n",
    "Information Criterion (BIC) which contains the variables AtBat, Hits, \n",
    "Walks, CRBI, Division and PutOuts.\n",
    "\n",
    "Randomly split the observed data into a training set (containing \n",
    "roughly 80% of all of the data) and a held-out test set (containing \n",
    "roughly 20% of all of the data).Calculate the predictive root-mean-\n",
    "square error (RMSE) for each of the four models. Which model appears \n",
    "to be most appropriate? Justify why this model is most appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will split the data into training and test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = hitters.drop(\"Salary\", 1)\n",
    "y = hitters[\"Salary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training and Test set \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "train = pd.concat([X_train, y_train], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(I) The full model with all 19 explanatory variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "302.05370688561362"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = smf.ols(formula = 'Salary ~ AtBat + Hits + HmRun +  Runs + RBI + Walks + Years + CAtBat + CHits + CHmRun + CRuns + CRBI + CWalks + League + Division + PutOuts + Assists + Errors + NewLeague', data=train).fit()\n",
    "pred1 = m1.predict(X_test)\n",
    "RMSE1 = np.sqrt(np.mean((y_test - pred1)**2))\n",
    "RMSE1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ii) The optimal model identified in part b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "294.47897359077996"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2 = smf.ols(formula = 'Salary ~ AtBat + Hits + Walks + CAtBat + CRuns + CRBI + CWalks + League + Division + PutOuts + Assists', data = train).fit()\n",
    "pred2 = m2.predict(X_test)\n",
    "RMSE2 = np.sqrt(np.mean((y_test - pred2)**2))\n",
    "RMSE2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(iii) The best stepwise selection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296.41386222544998"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3 = smf.ols(formula = 'Salary ~ CRBI + Hits + PutOuts + Division + AtBat +  Walks + CWalks + CRuns + CAtBat + Assists', data = train).fit()\n",
    "pred3 = m3.predict(X_test)\n",
    "RMSE3 = np.sqrt(np.mean((y_test - pred3)**2))\n",
    "RMSE3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(IV) Bayesian Information criterian. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "281.71062614102044"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m4 = smf.ols('Salary ~ AtBat + Hits + Walks + CRBI + Division + PutOuts', data = train).fit()\n",
    "pred4 =  m4.predict(X_test)\n",
    "RMSE4 = np.sqrt(np.mean((y_test - pred4)**2))\n",
    "RMSE4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output optimal model is the best model because of the lower RMSE, since lower RMSE is better. Also the answer and best model should change every time since training set will be different everytime. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(G) As in part (f), you must compare the predictive performance of the same four models, but here you must determine \n",
    "the predictive accuracy (predictive RMSE) by using 10-Fold Cross Validation. Which model appears to be most \n",
    "appropriate? Justify why this model is most appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(I) All explanatory variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sahiljain/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "306.41804889221157"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#K-fold cross validation \n",
    "numfolds = 10\n",
    "kf = KFold(n=263, n_folds=numfolds, shuffle = True)\n",
    "MSE = 0\n",
    "for train_indices, test_indices in kf:\n",
    "    train_X = X.ix[train_indices, :]; train_y = y[train_indices]\n",
    "    test_X = X.ix[test_indices, :]; test_y = y[test_indices]\n",
    "    training = pd.concat([train_X, train_y], axis = 1)\n",
    "    m5 = smf.ols('Salary ~ AtBat + Hits + HmRun +  Runs + RBI + Walks + Years + CAtBat + CHits + CHmRun + CRuns + CRBI + CWalks + League + Division + PutOuts + Assists + Errors + NewLeague', data = train).fit()   \n",
    "    pred = m5.predict(test_X)\n",
    "    MSE = MSE + np.mean((test_y - pred)**2)\n",
    "RMSE5 = np.sqrt(MSE/numfolds)\n",
    "RMSE5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) The optimal model in part b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "307.54704203239589"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numfolds = 10\n",
    "kf = KFold(n=263, n_folds=numfolds, shuffle = True)\n",
    "MSE = 0\n",
    "for train_indices, test_indices in kf:\n",
    "    train_X = X.ix[train_indices, :]; train_y = y[train_indices]\n",
    "    test_X = X.ix[test_indices, :]; test_y = y[test_indices]\n",
    "    training = pd.concat([train_X, train_y], axis = 1)\n",
    "    m6 = smf.ols('Salary ~ AtBat + Hits + Walks + CAtBat + CRuns + CRBI + CWalks + League + Division + PutOuts + Assists', data = train).fit()   \n",
    "    pred = m6.predict(test_X)\n",
    "    MSE = MSE + np.mean((test_y - pred)**2)\n",
    "RMSE6 = np.sqrt(MSE/numfolds)\n",
    "RMSE6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) The best subset model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "307.24918033691949"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#K-fold cross validation \n",
    "numfolds = 10\n",
    "kf = KFold(n=263, n_folds=numfolds, shuffle = True)\n",
    "MSE = 0\n",
    "for train_indices, test_indices in kf:\n",
    "    train_X = X.ix[train_indices, :]; train_y = y[train_indices]\n",
    "    test_X = X.ix[test_indices, :]; test_y = y[test_indices]\n",
    "    training = pd.concat([train_X, train_y], axis = 1)\n",
    "    m7 = smf.ols('Salary ~ CRBI + Hits + PutOuts + Division + AtBat +  Walks + CWalks + CRuns + CAtBat + Assists', data = train).fit()   \n",
    "    pred = m7.predict(test_X)\n",
    "    MSE = MSE + np.mean((test_y - pred)**2)\n",
    "RMSE7 = np.sqrt(MSE/numfolds)\n",
    "RMSE7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) Bayesian Information Criterian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "317.70730086272999"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numfolds = 10\n",
    "kf = KFold(n=263, n_folds=numfolds, shuffle = True)\n",
    "MSE = 0\n",
    "for train_indices, test_indices in kf:\n",
    "    train_X = X.ix[train_indices, :]; train_y = y[train_indices]\n",
    "    test_X = X.ix[test_indices, :]; test_y = y[test_indices]\n",
    "    training = pd.concat([train_X, train_y], axis = 1)\n",
    "    m8 = smf.ols('Salary ~ AtBat + Hits + Walks + CRBI + Division + PutOuts', data = train).fit()   \n",
    "    pred = m8.predict(test_X)\n",
    "    MSE = MSE + np.mean((test_y - pred)**2)\n",
    "RMSE8 = np.sqrt(MSE/numfolds)\n",
    "RMSE8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing K-Folds Cross validation of 10 folds, we can determine that the best subset model (stepwise - selection) appears to be the most appropriate model, because of the root mean square error (RMSE) value since smaller the RMSE better will the predictive accuracy of the model. Same in this case the answer/better model will vary with respect to training set.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(H) Given the estimates of predictive accuracy from parts (f) and (g) indicate which estimates you believe to be more accurate. In other words, indicate which\n",
    "validation approach (i.e., cross validation vs. k-fold cross validation) you believe will most accurately estimate the predictive capability of a model. Briefly explain\n",
    "your rationale.\n",
    "\n",
    "K-fold cross validation is the much better approach as compared to cross validation because if we are proceeding with normal validation, we are reducing the dataset size by almost around 20–30%. However, In case of k-fold cross validation there won't be any such kind of reduction in dataset size moreover by doing k-fold cross validation we can prevent over fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(G) Accounting for all of the analyses you’ve performed (i.e., multicollinearity, goodness-of-fit, and predictive accuracy), which model would you be most comfortable using? Briefly justify your choice. [Note: I’m not looking for a right or wrong answer here; I want to see that you can sensibly and eloquently justify your choice].\n",
    "\n",
    "I will prefer predictive accuracy model because in predictive accuracy one can easily do different regression (step-wise) and different validations (cross, K-fold) to understand the result in a better way. Also in predictive accuracy most of the statistical insignificant variables are dropped out of the model which makes prediction even more easier. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
